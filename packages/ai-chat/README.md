# @ai-chat/core

A focused AI chat client for handling AI requests and tool calling in Node.js applications.

## ğŸ¯ Core Purpose

`@ai-chat` is designed with a clear focus on core AI interaction:
1. **AI Request Processing** - Send messages to AI providers and handle responses
2. **Tool Calling Coordination** - Manage tool calls and results in AI conversations
3. **Provider Abstraction** - Unified interface for different AI providers

This package does **NOT** handle:
- âŒ Model discovery and selection (use `model-manager` packages)
- âŒ Provider configuration management (use `config-manager`)
- âŒ Conversation history management (use `context-manager`)
- âŒ Message persistence (use `context-manager`) 
- âŒ Session state tracking (use `context-manager`)
- âŒ Token calculation and cost estimation (use dedicated token calculation packages)
- âŒ Specific tool implementations (use `mcp-client` or custom providers)

## ğŸš€ Quick Start

```typescript
import { AIChat } from '@deepracticex/ai-chat'

// âœ¨ Simple and direct - specify exactly what you need
const aiChat = new AIChat({
  baseUrl: 'https://api.openai.com/v1',
  model: 'gpt-4',
  apiKey: process.env.OPENAI_API_KEY
})

// ğŸŒ Works with any OpenAI-compatible API
const claude = new AIChat({
  baseUrl: 'https://api.anthropic.com/v1',
  model: 'claude-3-sonnet-20240229',
  apiKey: process.env.CLAUDE_API_KEY
})

const azure = new AIChat({
  baseUrl: 'https://your-resource.openai.azure.com',
  model: 'gpt-4',
  apiKey: process.env.AZURE_OPENAI_KEY
})

const ollama = new AIChat({
  baseUrl: 'http://localhost:11434',
  model: 'llama3'
  // No API key needed for local services
})

// ğŸš€ Send streaming messages
for await (const chunk of aiChat.sendMessage(messages)) {
  if (chunk.content) process.stdout.write(chunk.content)
  if (chunk.done) break
}
```

## ğŸ“– Core API

### AIChat Class

```typescript
class AIChat {
  constructor(config: AIChatConfig)
  
  // Send message and get complete response
  sendMessage(
    messages: Message[], 
    options?: ChatOptions
  ): Promise<ChatResponse>
  
  // Send message and get streaming response
  sendMessageStream(
    messages: Message[],
    options?: ChatOptions  
  ): AsyncIterable<ChatStreamChunk>
}
```

### Simple Configuration

Direct and explicit configuration - no magic, no guessing:

```typescript
interface AIChatConfig {
  baseUrl: string   // API service endpoint URL - always required
  model: string     // Model name - always required
  apiKey?: string   // API key - optional for local services
  temperature?: number
  maxTokens?: number
}

// âœ… Examples - Clear and explicit
{
  baseUrl: 'https://api.openai.com/v1',
  model: 'gpt-4',
  apiKey: 'sk-...'
}

{
  baseUrl: 'https://api.anthropic.com/v1', 
  model: 'claude-3-sonnet-20240229',
  apiKey: 'sk-ant-...'
}

{
  baseUrl: 'http://localhost:11434',
  model: 'llama3'
  // No API key needed for local Ollama
}
```

## ğŸ¯ Provider and Model Management

**Models and providers are managed externally** - use dedicated packages for configuration:

```typescript
// âœ… Get configuration from external model management
import { getModelConfig } from '@deechat/model-manager'

const modelConfig = await getModelConfig({
  task: 'coding',
  preference: 'fastest' 
})

const aiChat = new AIChat(modelConfig)
// modelConfig = {
//   baseUrl: 'https://api.openai.com/v1',
//   model: 'gpt-4-turbo',
//   apiKey: '...'
// }

// âœ… Or use provider configuration helpers
import { openaiConfig, claudeConfig } from '@deechat/provider-configs'

const aiChat = new AIChat(
  openaiConfig('gpt-4', { apiKey: process.env.OPENAI_KEY })
)
```

### Tool Integration

```typescript
// Tools are provided as input, not discovered by this package
const response = await aiChat.sendMessage(messages, {
  tools: [
    {
      name: "search_files",
      description: "Search for files",
      parameters: { /* JSON Schema */ }
    }
  ],
  onToolCall: async (call) => {
    // Your tool execution logic here
    // This could call mcp-client, local functions, etc.
    return {
      toolCallId: call.id,
      result: await executeMyTool(call.name, call.arguments)
    }
  }
})
```

## ğŸŒŠ Streaming Example

```typescript
const stream = aiChat.sendMessageStream(messages, {
  tools: myTools,
  onToolCall: handleToolCall
})

for await (const chunk of stream) {
  if (chunk.content) {
    process.stdout.write(chunk.content)
  }
  
  if (chunk.toolCalls) {
    console.log('AI wants to call tools:', chunk.toolCalls)
  }
  
  if (chunk.done) {
    console.log('\nResponse complete!')
    break
  }
}
```


## ğŸ—ï¸ Architecture Integration

This package is designed to work alongside other focused packages:

```typescript
// Example: Complete DeeChat integration
import { AIChat } from '@ai-chat/core'
import { ContextManager } from '@context-manager'  
import { MCPClient } from '@mcp-client'

// Each package handles its own responsibility
const aiChat = new AIChat(aiConfig)           // AI communication
const contextManager = new ContextManager()   // History & state
const mcpClient = new MCPClient()             // Tool implementation

// Compose them together
const sessionId = 'session-123'
const history = contextManager.getMessages(sessionId)

const response = await aiChat.sendMessage(
  [...history, { role: 'user', content: userInput }],
  {
    tools: await mcpClient.getTools(),
    onToolCall: (call) => mcpClient.executeTools(call)
  }
)

// Update context with response
contextManager.addMessage(sessionId, response.message)
```

## ğŸ¯ Features

- **Multiple AI Providers**: OpenAI, Claude, Gemini support
- **Streaming Responses**: Real-time response streaming
- **Tool Calling**: Coordinate tool execution without managing tools
- **TypeScript First**: Full type safety and IntelliSense
- **Lightweight**: Focused scope, minimal dependencies
- **Framework Agnostic**: Works in any Node.js environment

## ğŸ“¦ Installation

```bash
npm install @ai-chat/core

# Peer dependencies (install the providers you need)
npm install openai anthropic  # for AI providers
```

## ğŸ“š Documentation

### æ ¸å¿ƒæ–‡æ¡£
- **[API è®¾è®¡æ–‡æ¡£](./docs/API_DESIGN.md)** - è¯¦ç»†çš„ API è®¾è®¡å’Œç±»å‹å®šä¹‰
- **[äº§å“éœ€æ±‚æ–‡æ¡£](./docs/prd.md)** - å®Œæ•´çš„äº§å“éœ€æ±‚å’Œç”¨æˆ·æ•…äº‹
- **[æ¶æ„è®¾è®¡](./docs/architecture.md)** - æŠ€æœ¯æ¶æ„å’Œè®¾è®¡å†³ç­–

### å¼€å‘æ–‡æ¡£  
- **[å¼€å‘æŒ‡å—](./docs/development.md)** - å¼€å‘ç¯å¢ƒæ­å»ºå’Œç¼–ç è§„èŒƒ
- **[æµ‹è¯•æŒ‡å—](./docs/testing.md)** - æµ‹è¯•ç­–ç•¥å’Œæµ‹è¯•ç”¨ä¾‹ç¼–å†™
- **[å˜æ›´æ—¥å¿—](./docs/CHANGELOG.md)** - ç‰ˆæœ¬å˜æ›´è®°å½•

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](./CONTRIBUTING.md).

## ğŸ“„ License

MIT License - see [LICENSE](./LICENSE) file for details.