/**
 * DeeChaté©±åŠ¨çš„AIå®¢æˆ·ç«¯
 * é›†æˆå®˜æ–¹DeeChatåŒ…ï¼Œæä¾›ç»Ÿä¸€çš„AIæ¥å£
 * DeepPractice Standards Compliant
 */

import { AIChat } from '@deepracticex/ai-chat';
import { countTokens, CostCalculator, TokenCalculator } from '@deepracticex/token-calculator';
import { ContextFormatter } from '@deepracticex/context-manager';
// import {
//   SocraticMessage,
//   SocraticRequest,
//   SocraticErrorCode
// } from '@/lib/types/socratic/ai-service';

// ä¸´æ—¶ç±»å‹å®šä¹‰ï¼Œè§£å†³å¯¼å…¥é—®é¢˜
type SocraticMessage = {
  role: string;
  content: string;
  timestamp?: string;
};

type SocraticRequest = {
  messages?: SocraticMessage[];
  level?: string;
  mode?: string;
  currentTopic?: string;
};

export interface DeeChatConfig {
  // AIæä¾›å•†é…ç½®
  provider: 'deepseek' | 'openai' | 'claude';
  apiKey: string;
  apiUrl?: string;
  model: string;

  // Tokenç®¡ç†é…ç½®
  maxContextTokens: number;
  reserveTokens: number;
  costThreshold: number; // æ¯æ¬¡è¯·æ±‚æœ€å¤§æˆæœ¬ï¼ˆç¾å…ƒï¼‰

  // è¡Œä¸ºé…ç½®
  temperature: number;
  enableStreaming: boolean;
  enableCostOptimization: boolean;
}

interface AIResponse {
  content: string;
  tokensUsed: {
    input: number;
    output: number;
    total: number;
  };
  cost: {
    input: number;
    output: number;
    total: number;
  };
  model: string;
  provider: string;
  duration: number;
}

interface StreamingResponse {
  stream: AsyncIterable<any>;
  metadata: {
    estimatedTokens: number;
    estimatedCost: number;
    model: string;
    provider: string;
  };
}

export class DeeChatAIClient {
  private aiChat: AIChat;
  private config: DeeChatConfig;
  private requestCount = 0;
  private totalCost = 0;

  constructor(config: DeeChatConfig) {
    this.config = config;
    this.aiChat = new AIChat({
      baseUrl: this.getBaseUrl(),
      model: config.model,
      apiKey: config.apiKey
    });
  }

  /**
   * è·å–AIæä¾›å•†çš„åŸºç¡€URL
   */
  private getBaseUrl(): string {
    if (this.config.apiUrl) return this.config.apiUrl;

    switch (this.config.provider) {
      case 'deepseek':
        return 'https://api.deepseek.com/v1';
      case 'openai':
        return 'https://api.openai.com/v1';
      case 'claude':
        return 'https://api.anthropic.com/v1';
      default:
        return 'https://api.deepseek.com/v1';
    }
  }

  /**
   * æ™ºèƒ½Tokenè®¡ç®— - ä¼˜åŒ–ä¸Šä¸‹æ–‡é•¿åº¦
   */
  async calculateOptimalTokens(messages: SocraticMessage[], contextString: string): Promise<{
    inputTokens: number;
    maxOutputTokens: number;
    isOptimal: boolean;
    suggestion?: string;
  }> {
    try {
      // è®¡ç®—è¾“å…¥tokenæ•°
      const inputTokens = countTokens(contextString, this.config.provider, this.config.model);

      // è®¡ç®—æœ€ä¼˜è¾“å‡ºtokenæ•°
      const availableTokens = this.config.maxContextTokens - inputTokens - this.config.reserveTokens;
      const maxOutputTokens = Math.max(100, Math.min(1000, availableTokens));

      const isOptimal = availableTokens > 200; // è‡³å°‘200ä¸ªtokenç”¨äºè¾“å‡º

      return {
        inputTokens,
        maxOutputTokens,
        isOptimal,
        suggestion: !isOptimal ? 'ä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œå»ºè®®ç¼©çŸ­å¯¹è¯å†å²' : undefined
      };
    } catch (error) {
      console.warn('Tokenè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼:', error);
      return {
        inputTokens: Math.floor(contextString.length / 4), // ç²—ç•¥ä¼°ç®—
        maxOutputTokens: 500,
        isOptimal: false,
        suggestion: 'æ— æ³•ç²¾ç¡®è®¡ç®—tokenï¼Œä½¿ç”¨ä¼°ç®—å€¼'
      };
    }
  }

  /**
   * æˆæœ¬é¢„ä¼°
   */
  async estimateRequestCost(inputTokens: number, outputTokens: number): Promise<{
    inputCost: number;
    outputCost: number;
    totalCost: number;
    withinBudget: boolean;
  }> {
    try {
      const tokenCalculator = new TokenCalculator();
      const costCalculator = new CostCalculator(tokenCalculator);
      const tokenUsage = {
        inputTokens,
        outputTokens,
        totalTokens: inputTokens + outputTokens
      };

      const costEstimate = costCalculator.estimateCost(tokenUsage, this.config.model);

      return {
        inputCost: costEstimate.inputCost,
        outputCost: costEstimate.outputCost,
        totalCost: costEstimate.totalCost,
        withinBudget: costEstimate.totalCost <= this.config.costThreshold
      };
    } catch (error) {
      console.warn('æˆæœ¬ä¼°ç®—å¤±è´¥:', error);
      return {
        inputCost: 0,
        outputCost: 0,
        totalCost: 0,
        withinBudget: true
      };
    }
  }

  /**
   * å‘é€æ¶ˆæ¯ - ç»Ÿä¸€æ¥å£
   */
  async sendMessage(contextString: string, request: SocraticRequest): Promise<AIResponse> {
    const startTime = Date.now();

    try {
      // Tokenä¼˜åŒ–
      const tokenInfo = await this.calculateOptimalTokens(
        request.messages || [],
        contextString
      );

      // æˆæœ¬æ£€æŸ¥
      if (this.config.enableCostOptimization) {
        const costEstimate = await this.estimateRequestCost(
          tokenInfo.inputTokens,
          tokenInfo.maxOutputTokens
        );

        if (!costEstimate.withinBudget) {
          throw new Error(`é¢„ä¼°æˆæœ¬ $${costEstimate.totalCost.toFixed(4)} è¶…å‡ºé˜ˆå€¼ $${this.config.costThreshold}`);
        }
      }

      // æ„å»ºæ¶ˆæ¯
      const messages = [
        {
          role: 'user' as const,
          content: contextString
        }
      ];

      // å‘é€è¯·æ±‚
      const response = await this.aiChat.sendMessageComplete(messages, {
        temperature: this.config.temperature,
        maxTokens: tokenInfo.maxOutputTokens
      });

      // è®¡ç®—å®é™…ä½¿ç”¨çš„tokenå’Œæˆæœ¬
      const outputTokens = countTokens(response.message.content, this.config.provider, this.config.model);
      const actualCost = await this.estimateRequestCost(tokenInfo.inputTokens, outputTokens);

      // æ›´æ–°ç»Ÿè®¡
      this.requestCount++;
      this.totalCost += actualCost.totalCost;

      const duration = Date.now() - startTime;

      return {
        content: response.message.content,
        tokensUsed: {
          input: tokenInfo.inputTokens,
          output: outputTokens,
          total: tokenInfo.inputTokens + outputTokens
        },
        cost: {
          input: actualCost.inputCost,
          output: actualCost.outputCost,
          total: actualCost.totalCost
        },
        model: this.config.model,
        provider: this.config.provider,
        duration
      };

    } catch (error) {
      console.error('DeeChat AIè°ƒç”¨å¤±è´¥:', error);
      throw error;
    }
  }

  /**
   * æµå¼å“åº” - å®æ—¶ç”Ÿæˆ
   */
  async sendMessageStream(contextString: string, request: SocraticRequest): Promise<StreamingResponse> {
    try {
      // Tokenå’Œæˆæœ¬é¢„ä¼°
      const tokenInfo = await this.calculateOptimalTokens(
        request.messages || [],
        contextString
      );

      const costEstimate = await this.estimateRequestCost(
        tokenInfo.inputTokens,
        tokenInfo.maxOutputTokens
      );

      if (this.config.enableCostOptimization && !costEstimate.withinBudget) {
        throw new Error(`é¢„ä¼°æˆæœ¬è¶…å‡ºé¢„ç®—`);
      }

      // æ„å»ºæ¶ˆæ¯
      const messages = [
        {
          role: 'user' as const,
          content: contextString
        }
      ];

      // å‘é€æµå¼è¯·æ±‚
      const stream = this.aiChat.sendMessage(messages, {
        temperature: this.config.temperature,
        maxTokens: tokenInfo.maxOutputTokens
      });

      return {
        stream,
        metadata: {
          estimatedTokens: tokenInfo.inputTokens + tokenInfo.maxOutputTokens,
          estimatedCost: costEstimate.totalCost,
          model: this.config.model,
          provider: this.config.provider
        }
      };

    } catch (error) {
      console.error('DeeChatæµå¼è°ƒç”¨å¤±è´¥:', error);
      throw error;
    }
  }

  /**
   * æ£€æŸ¥æœåŠ¡å¯ç”¨æ€§
   */
  isAvailable(): boolean {
    return Boolean(this.config.apiKey);
  }

  /**
   * è·å–ä½¿ç”¨ç»Ÿè®¡
   */
  getUsageStats() {
    return {
      requestCount: this.requestCount,
      totalCost: this.totalCost,
      averageCostPerRequest: this.requestCount > 0 ? this.totalCost / this.requestCount : 0,
      provider: this.config.provider,
      model: this.config.model
    };
  }

  /**
   * é‡ç½®ç»Ÿè®¡
   */
  resetStats() {
    this.requestCount = 0;
    this.totalCost = 0;
  }

  /**
   * æ›´æ–°é…ç½®
   */
  updateConfig(updates: Partial<DeeChatConfig>) {
    this.config = { ...this.config, ...updates };

    // æ›´æ–°AIChatå®ä¾‹
    this.aiChat = new AIChat({
      baseUrl: this.getBaseUrl(),
      model: this.config.model,
      apiKey: this.config.apiKey
    });
  }

  /**
   * å‘é€è‡ªå®šä¹‰æ¶ˆæ¯ - æ”¯æŒåŒæç¤ºè¯æ¨¡å¼ (System + User)
   * ä¸ºEnhancedSocraticServiceçš„æ¨¡å—åŒ–æ¶æ„æä¾›æ”¯æŒ
   */
  async sendCustomMessage(
    messages: Array<{ role: 'system' | 'user' | 'assistant', content: string }>,
    options?: {
      temperature?: number;
      maxTokens?: number;
      enableCostOptimization?: boolean;
    }
  ): Promise<AIResponse> {
    const startTime = Date.now();

    try {
      // å‚æ•°éªŒè¯
      if (!Array.isArray(messages)) {
        throw new Error(`DeeChatAIClient.sendCustomMessage: messageså‚æ•°å¿…é¡»æ˜¯æ•°ç»„ï¼Œå®é™…ç±»å‹: ${typeof messages}`);
      }

      // åˆå¹¶é…ç½®é€‰é¡¹
      const finalOptions = {
        temperature: options?.temperature || this.config.temperature,
        maxTokens: options?.maxTokens || 1000,
        enableCostOptimization: options?.enableCostOptimization ?? this.config.enableCostOptimization
      };

      // è®¡ç®—è¾“å…¥tokenæ•°ï¼ˆåˆå¹¶æ‰€æœ‰æ¶ˆæ¯å†…å®¹ï¼‰
      const combinedContent = messages.map(m => m.content).join('\n');
      const inputTokens = countTokens(combinedContent, this.config.provider, this.config.model);

      // æˆæœ¬æ£€æŸ¥
      if (finalOptions.enableCostOptimization) {
        const costEstimate = await this.estimateRequestCost(
          inputTokens,
          finalOptions.maxTokens
        );

        if (!costEstimate.withinBudget) {
          throw new Error(`é¢„ä¼°æˆæœ¬ $${costEstimate.totalCost.toFixed(4)} è¶…å‡ºé˜ˆå€¼ $${this.config.costThreshold}`);
        }
      }

      // å‘é€è¯·æ±‚åˆ°AIæœåŠ¡
      const response = await this.aiChat.sendMessageComplete(messages, {
        temperature: finalOptions.temperature,
        maxTokens: finalOptions.maxTokens
      });

      // è®¡ç®—å®é™…ä½¿ç”¨çš„tokenå’Œæˆæœ¬
      const outputTokens = countTokens(response.message.content, this.config.provider, this.config.model);
      const actualCost = await this.estimateRequestCost(inputTokens, outputTokens);

      // æ›´æ–°ç»Ÿè®¡
      this.requestCount++;
      this.totalCost += actualCost.totalCost;

      const duration = Date.now() - startTime;

      return {
        content: response.message.content,
        tokensUsed: {
          input: inputTokens,
          output: outputTokens,
          total: inputTokens + outputTokens
        },
        cost: {
          input: actualCost.inputCost,
          output: actualCost.outputCost,
          total: actualCost.totalCost
        },
        model: this.config.model,
        provider: this.config.provider,
        duration
      };

    } catch (error) {
      console.error('DeeChatè‡ªå®šä¹‰æ¶ˆæ¯è°ƒç”¨å¤±è´¥:', error);
      throw error;
    }
  }

  /**
   * å‘é€æµå¼è‡ªå®šä¹‰æ¶ˆæ¯ - æ”¯æŒåŒæç¤ºè¯æ¨¡å¼çš„æµå¼å“åº”
   */
  async sendCustomMessageStream(
    messages: Array<{ role: 'system' | 'user' | 'assistant', content: string }>,
    options?: {
      temperature?: number;
      maxTokens?: number;
      enableCostOptimization?: boolean;
    }
  ): Promise<StreamingResponse> {
    try {
      // åˆå¹¶é…ç½®é€‰é¡¹
      const finalOptions = {
        temperature: options?.temperature || this.config.temperature,
        maxTokens: options?.maxTokens || 1000,
        enableCostOptimization: options?.enableCostOptimization ?? this.config.enableCostOptimization
      };

      // è®¡ç®—è¾“å…¥tokenæ•°å’Œæˆæœ¬é¢„ä¼°
      const combinedContent = messages.map(m => m.content).join('\n');
      const inputTokens = countTokens(combinedContent, this.config.provider, this.config.model);
      const costEstimate = await this.estimateRequestCost(inputTokens, finalOptions.maxTokens);

      if (finalOptions.enableCostOptimization && !costEstimate.withinBudget) {
        throw new Error(`é¢„ä¼°æˆæœ¬è¶…å‡ºé¢„ç®—`);
      }

      // å‘é€æµå¼è¯·æ±‚
      const stream = this.aiChat.sendMessage(messages, {
        temperature: finalOptions.temperature,
        maxTokens: finalOptions.maxTokens
      });

      return {
        stream,
        metadata: {
          estimatedTokens: inputTokens + finalOptions.maxTokens,
          estimatedCost: costEstimate.totalCost,
          model: this.config.model,
          provider: this.config.provider
        }
      };

    } catch (error) {
      console.error('DeeChatè‡ªå®šä¹‰æµå¼è°ƒç”¨å¤±è´¥:', error);
      throw error;
    }
  }
}

// é»˜è®¤é…ç½®å·¥å‚å‡½æ•°
export function createDeeChatConfig(overrides: Partial<DeeChatConfig> = {}): DeeChatConfig {
  // ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„é…ç½®ï¼Œå…¶æ¬¡æ˜¯ç¯å¢ƒå˜é‡ï¼Œæœ€åæ˜¯é»˜è®¤å€¼
  const config = {
    provider: overrides.provider || 'deepseek' as const,
    apiKey: overrides.apiKey || process.env.DEEPSEEK_API_KEY || '',
    apiUrl: overrides.apiUrl || process.env.DEEPSEEK_API_URL || 'https://api.deepseek.com/v1',
    model: overrides.model || 'deepseek-chat',
    maxContextTokens: overrides.maxContextTokens || 8000,
    reserveTokens: overrides.reserveTokens || 100,
    costThreshold: overrides.costThreshold ?? 0.50, // å¢åŠ åˆ°50ç¾åˆ†ï¼Œç¡®ä¿AIåŠŸèƒ½æ­£å¸¸å·¥ä½œ
    temperature: overrides.temperature ?? 0.7,
    enableStreaming: overrides.enableStreaming ?? true,
    enableCostOptimization: overrides.enableCostOptimization ?? true,
  };

  // è°ƒè¯•ä¿¡æ¯
  console.log('ğŸ”§ DeeChatConfigåˆ›å»º:', {
    provider: config.provider,
    apiUrl: config.apiUrl,
    model: config.model,
    hasApiKey: !!config.apiKey,
    costThreshold: config.costThreshold
  });

  return config;
}